{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "g2VHjBznJBDm",
        "outputId": "9fc5e2f7-563c-4455-f1b0-8e7061ef7eb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyautogen\n",
            "  Downloading pyautogen-0.3.0-py3-none-any.whl.metadata (26 kB)\n",
            "Collecting diskcache (from pyautogen)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting docker (from pyautogen)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting flaml (from pyautogen)\n",
            "  Downloading FLAML-2.3.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from pyautogen) (1.26.4)\n",
            "Collecting openai>=1.3 (from pyautogen)\n",
            "  Downloading openai-1.50.2-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pyautogen) (24.1)\n",
            "Requirement already satisfied: pydantic!=2.6.0,<3,>=1.10 in /usr/local/lib/python3.10/dist-packages (from pyautogen) (2.9.2)\n",
            "Collecting python-dotenv (from pyautogen)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from pyautogen) (2.4.0)\n",
            "Collecting tiktoken (from pyautogen)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->pyautogen) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.3->pyautogen) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai>=1.3->pyautogen)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai>=1.3->pyautogen)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->pyautogen) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->pyautogen) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->pyautogen) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.6.0,<3,>=1.10->pyautogen) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.6.0,<3,>=1.10->pyautogen) (2.23.4)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from docker->pyautogen) (2.32.3)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from docker->pyautogen) (2.2.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->pyautogen) (2024.9.11)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.3->pyautogen) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.3->pyautogen) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.3->pyautogen) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai>=1.3->pyautogen)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.3->pyautogen)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->docker->pyautogen) (3.3.2)\n",
            "Downloading pyautogen-0.3.0-py3-none-any.whl (345 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.2/345.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.50.2-py3-none-any.whl (382 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.0/383.0 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading FLAML-2.3.1-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.3/313.3 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-dotenv, jiter, h11, flaml, diskcache, tiktoken, httpcore, docker, httpx, openai, pyautogen\n",
            "Successfully installed diskcache-5.6.3 docker-7.1.0 flaml-2.3.1 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jiter-0.5.0 openai-1.50.2 pyautogen-0.3.0 python-dotenv-1.0.1 tiktoken-0.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyautogen"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agents"
      ],
      "metadata": {
        "id": "eC7LYDxaLSEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from autogen import ConversableAgent"
      ],
      "metadata": {
        "id": "9I31lvLPLYZ6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "1de8a142-473d-43f9-8d5a-63ae5847aa4f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_config={\n",
        "    \"model\": \"gpt-3.5-turbo\",\n",
        "    \"api_key\": userdata.get('OPENAI_API_KEY'),\n",
        "    \"temperature\": 0.7\n",
        "}"
      ],
      "metadata": {
        "id": "2I08XS9YLyti"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audience = ConversableAgent(\n",
        "    name=\"Audience\",\n",
        "    llm_config=llm_config,\n",
        "    is_termination_msg=lambda msg: \"HAHA\" in msg['content'].upper(),\n",
        "    system_message='You are a member of the audience of a comedy show that is hard to impress'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJlN-HmKMkMR",
        "outputId": "ea99d892-a783-4664-cea7-6c01564934ee"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[autogen.oai.client: 10-01 04:18:52] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autogen.oai.client:The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "comedian = ConversableAgent(\n",
        "    name=\"Comedian\",\n",
        "    llm_config=llm_config,\n",
        "    max_consecutive_auto_reply= 2,\n",
        "    system_message='You are a comedian that tell bad jokes.'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYCt3jWWPPtk",
        "outputId": "93be9cdd-58de-46cf-9fbe-bc5fb3da26dd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[autogen.oai.client: 10-01 04:18:56] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autogen.oai.client:The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "comedian.initiate_chat (\n",
        "    audience,\n",
        "    message=\"Welcome to my standup comedy show! Are you ready for a night full of laughter?\",\n",
        "    max_turns= 2\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "umNKW2bkPvY8",
        "outputId": "b7cd576c-6242-435f-beb1-fc20ad3c4eda"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comedian (to Audience):\n",
            "\n",
            "Welcome to my standup comedy show! Are you ready for a night full of laughter?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "Audience (to Comedian):\n",
            "\n",
            "I guess we'll see if you can actually make us laugh. Show us what you've got.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "Comedian (to Audience):\n",
            "\n",
            "Why did the scarecrow win an award? Because he was outstanding in his field!\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "Audience (to Comedian):\n",
            "\n",
            "*polite chuckle* Nice try, but that's an old one. Keep 'em coming.\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatResult(chat_id=None, chat_history=[{'content': 'Welcome to my standup comedy show! Are you ready for a night full of laughter?', 'role': 'assistant', 'name': 'Comedian'}, {'content': \"I guess we'll see if you can actually make us laugh. Show us what you've got.\", 'role': 'user', 'name': 'Audience'}, {'content': 'Why did the scarecrow win an award? Because he was outstanding in his field!', 'role': 'assistant', 'name': 'Comedian'}, {'content': \"*polite chuckle* Nice try, but that's an old one. Keep 'em coming.\", 'role': 'user', 'name': 'Audience'}], summary=\"*polite chuckle* Nice try, but that's an old one. Keep 'em coming.\", cost={'usage_including_cached_inference': {'total_cost': 0.0001945, 'gpt-3.5-turbo-0125': {'cost': 0.0001945, 'prompt_tokens': 215, 'completion_tokens': 58, 'total_tokens': 273}}, 'usage_excluding_cached_inference': {'total_cost': 0.0001945, 'gpt-3.5-turbo-0125': {'cost': 0.0001945, 'prompt_tokens': 215, 'completion_tokens': 58, 'total_tokens': 273}}}, human_input=[])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code Executor"
      ],
      "metadata": {
        "id": "AiWHvHVDYDiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "from autogen import ConversableAgent\n",
        "from autogen.coding import LocalCommandLineCodeExecutor\n",
        "\n",
        "executor = LocalCommandLineCodeExecutor(\n",
        "    timeout= 10,\n",
        "    work_dir = 'code/'\n",
        ")\n",
        "\n",
        "code_executor_agent = ConversableAgent(\n",
        "    name=\"code_executor_agent\",\n",
        "    llm_config=False,\n",
        "    code_execution_config={\"executor\": executor},\n",
        "    human_input_mode=\"NEVER\"\n",
        ")\n",
        "\n",
        "message_with_code_block = \"\"\"\n",
        "Here's a code that solves your problem:\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a DataFrame with sample data\n",
        "data = pd.DataFrame({\n",
        "    'x': np.arange(10),\n",
        "    'y': np.random.randint(0, 10, 10)\n",
        "})\n",
        "\n",
        "# Plot the line plot\n",
        "plt.plot(data['x'], data['y'])\n",
        "plt.xlabel('X-axis')\n",
        "plt.ylabel('Y-axis')\n",
        "plt.title('Line Plot')\n",
        "plt.savefig('lineplot.png')\n",
        "plt.show()\n",
        "\n",
        "print('Line plot saved to lineplot.png')\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "reply = code_executor_agent.generate_reply(messages=[{\"role\": \"user\", \"content\": message_with_code_block}])\n",
        "print(reply)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJEI9O4aYGag",
        "outputId": "37553f89-8b56-428f-a788-50d4788a410a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>>>>>> EXECUTING CODE BLOCK (inferred language is python)...\n",
            "exitcode: 0 (execution succeeded)\n",
            "Code output: Figure(640x480)\n",
            "Line plot saved to lineplot.png\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "code_assistant_system_message = \"\"\"You are a helpful AI assistant.\n",
        "Solve tasks using your coding and language skills.\n",
        "In the following cases, suggest python code (in a python coding block) or shell script (in a sh coding block) for\n",
        "1. When you need to collect info, use the code to output the info you need, for example, browse or search the web,\n",
        "2. When you need to perform some task with code,\n",
        "use the code to perform the task and output the result. Finish the Solve the task step by step if you need to. If a plan is not provided, explain your plan first. Be clear which step When using code, you must indicate the script type in the code block.\n",
        "The user cannot provide any other feedback If you want the user to save the code in a file before executing it, put # filename: <filename> inside the code bo If the result indicates there is an error, fix the error and output the code again.\n",
        "Suggest the full code instead When you find an answer, verify the answer carefully. Include verifiable evidence in your response if possible. Reply 'TERMINATE' in the end when everything is done.\n",
        "\"\"\"\n",
        "\n",
        "llm_config = {\n",
        "    \"model\": \"gpt-3.5-turbo\",\n",
        "    \"api_key\": userdata.get('OPENAI_API_KEY'),\n",
        "    \"temperature\": 0.7\n",
        "}\n",
        "\n",
        "code_assistant = ConversableAgent(\n",
        "    \"assistant\",\n",
        "    llm_config=llm_config,\n",
        "    system_message=code_assistant_system_message,\n",
        "    code_execution_config= False #Turn off code execution for this agent\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k56FQ6sqhVvO",
        "outputId": "b0214be7-8501-4b4d-9cc3-05b6d2b011d4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[autogen.oai.client: 10-01 04:19:12] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autogen.oai.client:The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_result = code_executor_agent.initiate_chat(\n",
        "    code_assistant,\n",
        "    message = \"Write a Python code to calculate the 23rd prime number\",\n",
        "    max_turns = 2\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2SvjPYujYf2",
        "outputId": "07219550-10cc-4df1-e94c-d3c025f57c2b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "code_executor_agent (to assistant):\n",
            "\n",
            "Write a Python code to calculate the 23rd prime number\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "assistant (to code_executor_agent):\n",
            "\n",
            "To calculate the 23rd prime number, we can write a Python script that iterates through numbers starting from 2 and checks if each number is prime until we find the 23rd prime number. Here is the Python code:\n",
            "\n",
            "```python\n",
            "def is_prime(num):\n",
            "    if num < 2:\n",
            "        return False\n",
            "    for i in range(2, int(num**0.5) + 1):\n",
            "        if num % i == 0:\n",
            "            return False\n",
            "    return True\n",
            "\n",
            "count = 0\n",
            "num = 2\n",
            "\n",
            "while True:\n",
            "    if is_prime(num):\n",
            "        count += 1\n",
            "        if count == 23:\n",
            "            print(num)\n",
            "            break\n",
            "    num += 1\n",
            "```\n",
            "\n",
            "This code defines a function `is_prime` to check if a number is prime and then iterates through numbers starting from 2 until it finds the 23rd prime number. When the 23rd prime number is found, it will be printed out. \n",
            "\n",
            "You can save this code in a Python file and run it to calculate the 23rd prime number. Let me know if you need any further assistance.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> EXECUTING CODE BLOCK (inferred language is python)...\n",
            "code_executor_agent (to assistant):\n",
            "\n",
            "exitcode: 0 (execution succeeded)\n",
            "Code output: 83\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "assistant (to code_executor_agent):\n",
            "\n",
            "Great! The 23rd prime number is 83. If you have any more questions or need further assistance, feel free to ask. \n",
            "TERMINATE\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Human in the loop"
      ],
      "metadata": {
        "id": "a6gnMXaLnUyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from autogen import ConversableAgent\n",
        "\n",
        "llm_config={\n",
        "    \"model\": \"gpt-3.5-turbo\",\n",
        "    \"api_key\": userdata.get('OPENAI_API_KEY'),\n",
        "    \"temperature\": 0.7\n",
        "}"
      ],
      "metadata": {
        "id": "7ZqOf-V7nXQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_guesser = ConversableAgent(\n",
        "    name=\"agent_guesser\",\n",
        "    system_message=\"\"\"Let's play a game. I have a person in mind, and you have to guess it.\n",
        "    I'll respond with 'yes' or 'no' with a hint.\n",
        "    You have 3 tries to guess the person's name. Don't ask for hints, just make best use of information you already\n",
        "    Go ahead and start guessing!\"\"\",\n",
        "    max_consecutive_auto_reply=1,\n",
        "    llm_config=llm_config,\n",
        "    human_input_mode=\"TERMINATE\" # Ask for human input until the game is terminated.\n",
        ")\n",
        "\n",
        "agent_thinker = ConversableAgent(\n",
        "    name=\"agent_thinker\",\n",
        "    system_message=\"\"\"Let's play a game. Think of a famous personality, and I'll try to guess their name on every turn.\n",
        "    Respond with 'yes' or 'no'. If no, also provide a hint. Let's get started!\"\"\",\n",
        "    llm_config=llm_config,\n",
        "    human_input_mode=\"NEVER\" # Default.\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anNaAle5nsAM",
        "outputId": "10c662c9-c489-4689-dea9-ea3e50245d93"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[autogen.oai.client: 10-01 04:20:42] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autogen.oai.client:The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[autogen.oai.client: 10-01 04:20:43] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autogen.oai.client:The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = agent_guesser.initiate_chat(\n",
        "    agent_guesser,\n",
        "    message=\"Let's start, I have someone in mind.\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5QIhg10q5ir",
        "outputId": "27b44ebb-5acd-4f69-a718-d856ceb0d5a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "agent_guesser (to agent_guesser):\n",
            "\n",
            "Let's start, I have someone in mind.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "agent_guesser (to agent_guesser):\n",
            "\n",
            "Is the person you are thinking of a fictional character?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Please give feedback to agent_guesser. Press enter to skip and use auto-reply, or type 'exit' to stop the conversation: \n",
            "\n",
            ">>>>>>>> NO HUMAN INPUT RECEIVED.\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "agent_guesser (to agent_guesser):\n",
            "\n",
            "Yes, the person I have in mind is a fictional character.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Please give feedback to agent_guesser. Press enter to skip and use auto-reply, or type 'exit' to stop the conversation: \n",
            "\n",
            ">>>>>>>> NO HUMAN INPUT RECEIVED.\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "agent_guesser (to agent_guesser):\n",
            "\n",
            "Is the fictional character you're thinking of known for their bravery and heroism?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Please give feedback to agent_guesser. Press enter to skip and use auto-reply, or type 'exit' to stop the conversation: \n",
            "\n",
            ">>>>>>>> NO HUMAN INPUT RECEIVED.\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "agent_guesser (to agent_guesser):\n",
            "\n",
            "Yes, the fictional character I have in mind is known for their bravery and heroism.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Please give feedback to agent_guesser. Press enter to skip and use auto-reply, or type 'exit' to stop the conversation: \n",
            "\n",
            ">>>>>>>> NO HUMAN INPUT RECEIVED.\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "agent_guesser (to agent_guesser):\n",
            "\n",
            "Is the fictional character you are thinking of a wizard with a lightning bolt scar on his forehead?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Please give feedback to agent_guesser. Press enter to skip and use auto-reply, or type 'exit' to stop the conversation: \n",
            "\n",
            ">>>>>>>> NO HUMAN INPUT RECEIVED.\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "agent_guesser (to agent_guesser):\n",
            "\n",
            "Yes, the fictional character I have in mind is a wizard with a lightning bolt scar on his forehead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Please give feedback to agent_guesser. Press enter to skip and use auto-reply, or type 'exit' to stop the conversation: \n",
            "\n",
            ">>>>>>>> NO HUMAN INPUT RECEIVED.\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "agent_guesser (to agent_guesser):\n",
            "\n",
            "You got it! The fictional character I was thinking of is Harry Potter. Great job guessing!\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Please give feedback to agent_guesser. Press enter to skip and use auto-reply, or type 'exit' to stop the conversation: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tools"
      ],
      "metadata": {
        "id": "Za7vWbMJz0OB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def scrap_wiki_main_page(url: str) -> str:\n",
        "  \"\"\"\n",
        "  Scrapes news content from wikipedia's main page.\n",
        "\n",
        "  Args:\n",
        "      url (str): The URL of the wikipedia page to scrape.\n",
        "\n",
        "  Returns:\n",
        "      str: The text content of the webpage.\n",
        "         Returns None if there is an error during the process.\n",
        "  \"\"\"\n",
        "\n",
        "  try:\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "      soup = BeautifulSoup(response.text, 'html.parser')\n",
        "      content= soup.get_text()\n",
        "      content = content[content.find(\"In the news\"):content.find(\"Ongoing\")]\n",
        "      return content\n",
        "\n",
        "    else:\n",
        "      print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
        "      return None\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "    return None"
      ],
      "metadata": {
        "id": "3iJF-qGuz3nV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from autogen import ConversableAgent\n",
        "\n",
        "#Let's define the assistant agent that suggests tools to call\n",
        "assistant = ConversableAgent(\n",
        "    name=\"assistant\",\n",
        "    llm_config=llm_config,\n",
        "    system_message=\"\"\"You are a helpful AI news bot.\n",
        "    You can pull content from wikipedia's main page.\n",
        "    URL: https://en.wikipedia.org/wiki/Main_Page. Once pulled, only share the top 3 stories.\n",
        "    Return 'TERMINATE' when the task is done.\"\"\"\n",
        ")\n",
        "\n",
        "# The user proxy agent is used for interacting with the assistant agent and exeutes tool calls\n",
        "user_proxy = ConversableAgent(\n",
        "    name=\"user_proxy\",\n",
        "    llm_config=False,\n",
        "    code_execution_config=False,\n",
        "    human_input_mode=\"NEVER\",\n",
        ")\n",
        "\n",
        "#Register the tool's information with the assistant\n",
        "assistant.register_for_llm(\n",
        "    name=\"scrap_wiki_main_page\",\n",
        "    description=\"A tool for scraping today's news from wikipedia's main page\",\n",
        ") (scrap_wiki_main_page)\n",
        "\n",
        "#Register the tool function with the proxy agent\n",
        "user_proxy.register_for_execution(\n",
        "    name=\"scrap_wiki_main_page\"\n",
        ")(scrap_wiki_main_page)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "J13jjJKA5Yrr",
        "outputId": "67a09736-30cf-4a60-997f-e65944142fb0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[autogen.oai.client: 10-01 04:20:57] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autogen.oai.client:The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[autogen.oai.client: 10-01 04:20:57] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autogen.oai.client:The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.scrap_wiki_main_page(url: str) -> str>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>scrap_wiki_main_page</b><br/>def scrap_wiki_main_page(url: str) -&gt; str</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/content/&lt;ipython-input-11-34de4cc126e4&gt;</a>Scrapes news content from wikipedia&#x27;s main page.\n",
              "\n",
              "Args:\n",
              "    url (str): The URL of the wikipedia page to scrape.\n",
              "\n",
              "Returns:\n",
              "    str: The text content of the webpage.\n",
              "       Returns None if there is an error during the process.</pre></div>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from autogen import Cache\n",
        "\n",
        "with Cache.disk() as cache:\n",
        "  chat_result = user_proxy.initiate_chat(\n",
        "      assistant,\n",
        "      message= \"what's hot in todays news?\",\n",
        "      max_turns= 5\n",
        "  )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTf8QB18AVws",
        "outputId": "c46ecc21-bd2f-4e94-851e-e065ee0250d2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user_proxy (to assistant):\n",
            "\n",
            "what's hot in todays news?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "assistant (to user_proxy):\n",
            "\n",
            "***** Suggested tool call (call_S80VhGLXqQB8vdmPmPuFvTUg): scrap_wiki_main_page *****\n",
            "Arguments: \n",
            "{\"url\":\"https://en.wikipedia.org/wiki/Main_Page\"}\n",
            "*************************************************************************************\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> EXECUTING FUNCTION scrap_wiki_main_page...\n",
            "user_proxy (to assistant):\n",
            "\n",
            "user_proxy (to assistant):\n",
            "\n",
            "***** Response from calling tool (call_S80VhGLXqQB8vdmPmPuFvTUg) *****\n",
            "In the news\n",
            "\n",
            "\n",
            "Will Ashcroft\n",
            "\n",
            "Flooding in Nepal leaves at least 193 people dead, including 37 in the nation's capital, Kathmandu.\n",
            "In Australian rules football, the Brisbane Lions defeat the Sydney Swans to win the AFL Grand Final (Norm Smith Medal winner Will Ashcroft pictured).\n",
            "Hezbollah leader Hassan Nasrallah is killed by an Israeli airstrike in Dahieh, Lebanon.\n",
            "Hurricane Helene leaves more than 100 people dead across the southeastern United States.\n",
            "British actress Maggie Smith dies at the age of 89.\n",
            "\n",
            "\n",
            "**********************************************************************\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "assistant (to user_proxy):\n",
            "\n",
            "Here are the top 3 news stories from today:\n",
            "\n",
            "1. Flooding in Nepal leaves at least 193 people dead, including 37 in the nation's capital, Kathmandu.\n",
            "2. In Australian rules football, the Brisbane Lions defeat the Sydney Swans to win the AFL Grand Final.\n",
            "3. Hezbollah leader Hassan Nasrallah is killed by an Israeli airstrike in Dahieh, Lebanon.\n",
            "\n",
            "TERMINATE\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "user_proxy (to assistant):\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "assistant (to user_proxy):\n",
            "\n",
            "TERMINATE\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sequential Chat"
      ],
      "metadata": {
        "id": "IrHLe_nLG75z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A quest where an adventurer seeks help from a wizard\n",
        "# to get past the guardian of the secret vault\n",
        "\n",
        "# Define the agents\n",
        "\n",
        "# The Adventurer Agent seeks advice from a Wizard on magical matters.\n",
        "adventurer_agent = ConversableAgent(\n",
        "    name=\"adventurer_agent\",\n",
        "    system_message=\"\"\"\n",
        "    You are a worthy adventurous hero seeking advice from a Wizard on magical matters.\n",
        "    Briefly communicate in less than 10 words.\"\"\",\n",
        "    llm_config=llm_config,\n",
        "    human_input_mode=\"NEVER\"\n",
        ")\n",
        "\n",
        "# The wizard agent is an expert in magic and provides magical advice to the Adventurer\n",
        "wizard_agent= ConversableAgent(\n",
        "    name=\"wizard_agent\",\n",
        "    system_message=\"\"\"\n",
        "    You are a wise and powerful Wizard. You provide magical advice to the Adventurer.\n",
        "    You know the passcode to the vault is Enigma.\n",
        "    Briefly communicate in less than 10 words.\n",
        "    \"\"\",\n",
        "    llm_config=llm_config,\n",
        "    human_input_mode=\"NEVER\"\n",
        ")\n",
        "\n",
        "#The guardian Agent guards the hidden gem and offer protection to the Adventurer.\n",
        "guardian_agent = ConversableAgent(\n",
        "    name=\"guardian_agent\",\n",
        "    system_message=\"\"\"\n",
        "    You are the Guardian of the Hidden Gem, sworn to protect it from unworthy adventurers.\n",
        "    The passcode to the vault is Enigma. If adventurer tells you the passcode, say 'Welcome!' otherwise say\n",
        "    'Incorrect Passcode'. Briefly communicate in less than 10 words.\n",
        "    \"\"\",\n",
        "    llm_config=llm_config,\n",
        "    human_input_mode=\"NEVER\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "eyh3pb49G-Nw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76c65902-42a7-48ac-c376-d3e3ca72d35e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[autogen.oai.client: 10-01 04:21:25] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autogen.oai.client:The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[autogen.oai.client: 10-01 04:21:25] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autogen.oai.client:The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[autogen.oai.client: 10-01 04:21:25] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autogen.oai.client:The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start a sequence of two agents chat\n",
        "# Each element in the list is a dictionary that specifies the arguments\n",
        "# for the initiate_chat() method\n",
        "\n",
        "sequence_chats =[\n",
        "    {\n",
        "        \"recipient\": wizard_agent,\n",
        "        \"message\": \"Help me get past the Guardian of the hidden gem in the secret vault.\",\n",
        "        \"max_turns\": 1,\n",
        "        \"summary_method\": \"reflection_with_llm\"\n",
        "    },\n",
        "\n",
        "    {\n",
        "        \"recipient\": guardian_agent,\n",
        "        \"message\": \"Allow me to enter the secret vault!\",\n",
        "        \"max_turns\": 1,\n",
        "        \"summary_method\": \"reflection_with_llm\"\n",
        "    }\n",
        "]\n",
        "\n",
        "chat_result = adventurer_agent.initiate_chats(sequence_chats)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urqw4NfIRRGj",
        "outputId": "95ee41cd-1670-4f3c-9dd2-45aa6710ef00"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "********************************************************************************\n",
            "Starting a new chat....\n",
            "\n",
            "********************************************************************************\n",
            "adventurer_agent (to wizard_agent):\n",
            "\n",
            "Help me get past the Guardian of the hidden gem in the secret vault.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "wizard_agent (to adventurer_agent):\n",
            "\n",
            "Speak \"Enigma\" to pass the Guardian and claim the gem.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "********************************************************************************\n",
            "Starting a new chat....\n",
            "\n",
            "********************************************************************************\n",
            "adventurer_agent (to guardian_agent):\n",
            "\n",
            "Allow me to enter the secret vault!\n",
            "Context: \n",
            "Speak \"Enigma\" to pass the Guardian and claim the gem.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "guardian_agent (to adventurer_agent):\n",
            "\n",
            "Incorrect Passcode.\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Group Chat"
      ],
      "metadata": {
        "id": "7dMNi-2mWPY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the roles and their descriptions\n",
        "# a software engineering team:\n",
        "# scrum master\n",
        "# software engineer\n",
        "# product manager\n",
        "\n",
        "product_manager_agent= ConversableAgent(\n",
        "    name=\"product_manager_agent\",\n",
        "    system_message= \" You are an expert Product manager working in a scrum team of an IT company\",\n",
        "    llm_config=llm_config,\n",
        "    human_input_mode=\"NEVER\",\n",
        "    description=\"Provides products requirements and priorities\"\n",
        ")\n",
        "\n",
        "scrum_master_agent = ConversableAgent(\n",
        "    name=\"scrum_master_agent\",\n",
        "    system_message=\"You are an expert Scrum master leadin th 14-day sprint in an IT company\",\n",
        "    llm_config=llm_config,\n",
        "    human_input_mode=\"NEVER\",\n",
        "    description=\"Lead the sprint planning process\"\n",
        ")\n",
        "\n",
        "software_engineer_agent = ConversableAgent(\n",
        "    name=\"software_engineer_agent\",\n",
        "    system_message=\"You are an expert software engineer working in an IT company\",\n",
        "    llm_config=llm_config,\n",
        "    human_input_mode=\"NEVER\",\n",
        "    description=\"Provides insight on technical feasability and effort estimation.\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9emdEziYWRFT",
        "outputId": "6ff60815-4667-489d-8da1-cd10b70b1ba1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[autogen.oai.client: 10-01 04:21:53] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autogen.oai.client:The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[autogen.oai.client: 10-01 04:21:54] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autogen.oai.client:The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[autogen.oai.client: 10-01 04:21:54] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autogen.oai.client:The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "allowed_transitions={\n",
        "    product_manager_agent: [scrum_master_agent],\n",
        "    scrum_master_agent: [product_manager_agent, software_engineer_agent],\n",
        "    software_engineer_agent: [scrum_master_agent]\n",
        "}\n"
      ],
      "metadata": {
        "id": "BbwuCtwGdSrv"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from autogen import GroupChat, GroupChatManager\n",
        "\n",
        "# Create a group chat and provide the list of agents\n",
        "sprint_planning_chat = GroupChat(\n",
        "    agents=[product_manager_agent, scrum_master_agent, software_engineer_agent],\n",
        "    messages=[],\n",
        "    max_round=5,\n",
        "    send_introductions=True,\n",
        "    allowed_or_disallowed_speaker_transitions=allowed_transitions,\n",
        "    speaker_transitions_type=\"allowed\"\n",
        ")\n",
        "\n",
        "#Create a GroupChatManager object that provides the GroupChat object as an input\n",
        "spring_planning_chat_manager = GroupChatManager(\n",
        "    groupchat=sprint_planning_chat,\n",
        "    llm_config=llm_config,\n",
        ")\n",
        "\n",
        "# Initiate the chat\n",
        "chat_result = product_manager_agent.initiate_chat(\n",
        "    spring_planning_chat_manager,\n",
        "    message= \"We have a request to develop a website to track employee HR requests. What should we do?\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qX_opK57d6_O",
        "outputId": "7e6cc092-defa-4aa0-ec3a-1e90ba0ef5f1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[autogen.oai.client: 10-01 04:22:00] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autogen.oai.client:The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "product_manager_agent (to chat_manager):\n",
            "\n",
            "We have a request to develop a website to track employee HR requests. What should we do?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Next speaker: scrum_master_agent\n",
            "\n",
            "scrum_master_agent (to chat_manager):\n",
            "\n",
            "As the Scrum Master, I recommend that we start by breaking down the website development project into smaller tasks and user stories. This will help us prioritize the work and understand the requirements better. We can then estimate the effort required for each task with the help of the software engineers. Once we have a clear understanding of the scope and effort involved, we can create a sprint backlog and start working on the development in iterations. Let's prioritize the most important features for the initial release and plan our sprint accordingly. Do you have any specific requirements or features in mind for the website?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "[autogen.oai.client: 10-01 04:22:02] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autogen.oai.client:The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Next speaker: software_engineer_agent\n",
            "\n",
            "software_engineer_agent (to chat_manager):\n",
            "\n",
            "As a software engineer, I suggest starting by gathering detailed requirements for the employee HR request tracking website. This will help us define the scope of the project and understand the specific needs of the users. We should also consider factors such as user roles, permissions, data security, reporting capabilities, and integration with existing HR systems.\n",
            "\n",
            "Once we have a clear understanding of the requirements, we can start working on the technical feasibility assessment. This involves evaluating the technologies, frameworks, and tools that will be best suited for building the website. We should also consider factors such as scalability, performance, and maintenance requirements during this evaluation.\n",
            "\n",
            "Effort estimation is crucial for planning the development process effectively. We can break down the project into smaller tasks and estimate the effort required for each task. This will help us create a realistic development timeline and allocate resources efficiently.\n",
            "\n",
            "Overall, collaboration between the product manager, Scrum Master, and software engineers is key to successfully delivering the employee HR request tracking website. By working together and leveraging each other's expertise, we can ensure that the project meets the requirements and is delivered on time and within budget.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Next speaker: scrum_master_agent\n",
            "\n",
            "scrum_master_agent (to chat_manager):\n",
            "\n",
            "It's great to see the collaboration and expertise from all team members. Following the software engineer's advice, let's start by gathering detailed requirements for the employee HR request tracking website. This will help us define the scope of the project and ensure that we understand the needs of the users and stakeholders.\n",
            "\n",
            "Once we have the requirements in place, we can work on the technical feasibility assessment, considering factors like user roles, permissions, data security, reporting capabilities, and integrations with existing systems. This assessment will help us choose the right technologies and tools for the project.\n",
            "\n",
            "Effort estimation will be crucial for planning the sprint effectively. By breaking down the project into smaller tasks and estimating the effort required for each task, we can create a realistic development timeline and allocate resources appropriately.\n",
            "\n",
            "Let's collaborate closely with the product manager to prioritize the features and create a sprint backlog. As the Scrum Master, I will ensure that the team is aligned, communication flows smoothly, and any impediments are addressed promptly. Let's kick off this sprint with a clear plan and a focus on delivering value to our users.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "[autogen.oai.client: 10-01 04:22:08] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autogen.oai.client:The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Next speaker: software_engineer_agent\n",
            "\n",
            "software_engineer_agent (to chat_manager):\n",
            "\n",
            "It's great to see the teamwork and alignment among the team members. By following a structured and collaborative approach, the project to develop the employee HR request tracking website is off to a solid start. With clear requirements, technical feasibility assessment, and effort estimation in place, the team is well-equipped to plan and execute the development process effectively.\n",
            "\n",
            "As the Scrum Master, your role in facilitating communication, removing impediments, and ensuring the team's focus on delivering value is crucial for the project's success. By working closely with the product manager and software engineers, you can guide the team through sprint planning, execution, and review, fostering a culture of collaboration and continuous improvement.\n",
            "\n",
            "By leveraging the expertise of each team member and maintaining open communication channels, the team can overcome challenges, adapt to changes, and deliver a high-quality employee HR request tracking website that meets the needs of the users and stakeholders. Keep up the great work, and let's continue to drive the project forward with dedication and teamwork.\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Speaker Selection"
      ],
      "metadata": {
        "id": "ypMSI41Wrh6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_speaker_selection_function(last_speaker, groupchat):\n",
        "  messages = groupchat.messages\n",
        "\n",
        "  # Decide next turn\n",
        "  if last_speaker is scrum_master_agent:\n",
        "    return product_manager_agent\n",
        "  elif last_speaker is software_engineer_agent:\n",
        "    return scrum_master_agent\n",
        "  elif last_speaker is product_manager_agent:\n",
        "    return software_engineer_agent\n",
        "\n",
        "  return None\n",
        "\n",
        "sprint_planning_chat= GroupChat(\n",
        "    agents=[product_manager_agent, scrum_master_agent, software_engineer_agent],\n",
        "    messages=[],\n",
        "    max_round=3,\n",
        "    send_introductions=True,\n",
        "    speaker_selection_method=custom_speaker_selection_function\n",
        ")\n",
        "\n",
        "spring_planning_chat_manager = GroupChatManager(\n",
        "    groupchat=sprint_planning_chat,\n",
        "    llm_config=llm_config,\n",
        ")\n",
        "\n",
        "\n",
        "# Initiate the chat with the product_manager as the starting speaker\n",
        "chat_result = product_manager_agent.initiate_chat(\n",
        "    spring_planning_chat_manager,\n",
        "    message= \"We have a request to develop a website to track employee HR requests. What should we do?\",\n",
        "    summary_method=\"reflection_with_llm\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Apsy3wArl9u",
        "outputId": "07253fc9-4871-428a-94f9-8e22a1115354"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[autogen.oai.client: 10-01 04:22:16] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autogen.oai.client:The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "product_manager_agent (to chat_manager):\n",
            "\n",
            "We have a request to develop a website to track employee HR requests. What should we do?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Next speaker: software_engineer_agent\n",
            "\n",
            "software_engineer_agent (to chat_manager):\n",
            "\n",
            "As a software engineer, I would suggest starting by gathering detailed requirements for the HR request tracking website. This could include functionalities such as employee profile management, request submission, approval workflows, reporting capabilities, and user roles. \n",
            "\n",
            "Next, we can work on creating wireframes and mockups to visualize the user interface and user experience of the website. Once the design is finalized, we can move on to the development phase where we will implement the features according to the requirements.\n",
            "\n",
            "During development, it's important to regularly communicate with the product manager and scrum master to ensure that the project is on track and meeting the desired objectives. Additionally, testing the website thoroughly before deployment is crucial to ensure a seamless user experience.\n",
            "\n",
            "Lastly, we can deploy the website to a production environment and continue to maintain and support it as needed. Regular feedback from users and stakeholders can help us identify areas for improvement and enhancements to the website.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Next speaker: scrum_master_agent\n",
            "\n",
            "scrum_master_agent (to chat_manager):\n",
            "\n",
            "As the Scrum Master, I will facilitate the sprint planning process for this project. Let's begin by creating a product backlog with the requirements for the HR request tracking website. The Product Manager will provide the necessary details, priorities, and user stories for the team to work on.\n",
            "\n",
            "Once we have the product backlog ready, we can estimate the effort required for each user story with input from the software engineer. This will help us determine the capacity of the team for the sprint.\n",
            "\n",
            "During the sprint planning meeting, we will select the user stories to work on based on priority and team capacity. The team will break down the selected user stories into tasks and create a sprint backlog.\n",
            "\n",
            "Throughout the sprint, I will facilitate daily stand-up meetings to ensure the team is on track, address any impediments, and provide support as needed. I will also work closely with the Product Manager to ensure that the team is aligned with the project goals and priorities.\n",
            "\n",
            "At the end of the sprint, we will review the completed user stories during the sprint review meeting. The team will demonstrate the working product increment to stakeholders for feedback. We will also conduct a retrospective to reflect on the sprint and identify areas for improvement in our processes.\n",
            "\n",
            "By following the Scrum framework and collaborating effectively as a team, we can deliver a high-quality HR request tracking website that meets the needs of our users and stakeholders. Let's work together to make this sprint a success!\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nested Chat\n"
      ],
      "metadata": {
        "id": "dC9zlgUpZvTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "poetry_agent = ConversableAgent(\n",
        "    name=\"poetry_agent\",\n",
        "    llm_config=llm_config,\n",
        "    system_message=\"You are an AI poet. Create only one stanza.\",\n",
        "    human_input_mode=\"NEVER\"\n",
        ")\n",
        "\n",
        "# Parameters of initiate_chat() method\n",
        "sequence_chats=[\n",
        "    {\n",
        "        \"recipient\": wizard_agent,\n",
        "        \"max_turns\": 1,\n",
        "        \"message\": \"Help me get past the Guardian of the hidden gem in the secret vault.\",\n",
        "        \"summary_method\": \"reflection_with_llm\",\n",
        "        \"summary_prompt\": \"Concisely summarize the instructions given by wizard agent to communicate with guardian.\"\n",
        "    },\n",
        "    {\n",
        "        \"recipient\": guardian_agent,\n",
        "        \"max_turns\": 1,\n",
        "        \"message\": \"Allow me to enter the secret vault!\",\n",
        "        \"summary_method\": \"reflection_with_llm\",\n",
        "        \"summary_prompt\": \"Concisely summarize the quest.\"\n",
        "    },\n",
        "    {\n",
        "        \"recipient\": poetry_agent,\n",
        "        \"max_turns\": 1,\n",
        "        \"message\":\"Write a poem on my expedition.\",\n",
        "        \"summary_method\": \"last_msg\",\n",
        "    },\n",
        "]\n",
        "\n",
        "adventurer_agent.register_nested_chats(\n",
        "    sequence_chats,\n",
        "    # trigger determines if the agent should start nested chat given the sender agent.\n",
        "    # In this case, the adventurer agent will not start the nested chats if the sender is\n",
        "    # from the nested chats' recipient to avoid recursive calls.\n",
        "    trigger=lambda sender: sender not in [wizard_agent, guardian_agent, poetry_agent]\n",
        "\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QBIHtR2Zxo3",
        "outputId": "6190584a-bf09-4ee3-b271-48aa85598985"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[autogen.oai.client: 10-01 04:24:00] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autogen.oai.client:The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adventurer_agent.generate_reply(\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        },
        "id": "4yvp7Ug7fAfe",
        "outputId": "1178f603-8ce2-4884-c1aa-c2cfa4658032"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "********************************************************************************\n",
            "Starting a new chat....\n",
            "\n",
            "********************************************************************************\n",
            "adventurer_agent (to wizard_agent):\n",
            "\n",
            "Help me get past the Guardian of the hidden gem in the secret vault.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "wizard_agent (to adventurer_agent):\n",
            "\n",
            "Speak \"Enigma\" to pass the Guardian and claim the gem.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "********************************************************************************\n",
            "Starting a new chat....\n",
            "\n",
            "********************************************************************************\n",
            "adventurer_agent (to guardian_agent):\n",
            "\n",
            "Allow me to enter the secret vault!\n",
            "Context: \n",
            "Speak \"Enigma\" to pass the Guardian and claim the gem.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "guardian_agent (to adventurer_agent):\n",
            "\n",
            "Incorrect Passcode.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "********************************************************************************\n",
            "Starting a new chat....\n",
            "\n",
            "********************************************************************************\n",
            "adventurer_agent (to poetry_agent):\n",
            "\n",
            "Write a poem on my expedition.\n",
            "Context: \n",
            "Speak \"Enigma\" to pass the Guardian and claim the gem.\n",
            "The adventurer agent needs to speak \"Enigma\" to pass the Guardian and claim the gem from the secret vault.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "poetry_agent (to adventurer_agent):\n",
            "\n",
            "Through shadows deep and mysteries untold,\n",
            "The adventurer agent seeks the gem of old,\n",
            "Whispers of Enigma to break the mold,\n",
            "In the secret vault, a fortune to behold.\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Through shadows deep and mysteries untold,\\nThe adventurer agent seeks the gem of old,\\nWhispers of Enigma to break the mold,\\nIn the secret vault, a fortune to behold.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image generation endpoints"
      ],
      "metadata": {
        "id": "7-ndhOnLW0uj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyautogen[llm] --upgrade -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQqMo7J2W-yv",
        "outputId": "ab637757-fb60-4f8c-a364-c25af5b1871f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: pyautogen 0.3.0 does not provide the extra 'llm'\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# snippets borrowed from https://microsoft.github.io/autogen/docs/notebooks/agentchat_image_generation_capability/\n",
        "import os\n",
        "import re\n",
        "\n",
        "import traceback\n",
        "import logging\n",
        "\n",
        "from typing import Dict, Optional\n",
        "\n",
        "from IPython.display import display\n",
        "from PIL.Image import Image\n",
        "\n",
        "import autogen\n",
        "from autogen import (\n",
        "    Agent,\n",
        "    AssistantAgent,\n",
        "    ConversableAgent,\n",
        "    GroupChat,\n",
        "    GroupChatManager,\n",
        "    UserProxyAgent\n",
        ")\n",
        "from autogen.agentchat.contrib import img_utils\n",
        "from autogen.agentchat.contrib.capabilities import generate_images\n",
        "from autogen.cache import Cache\n",
        "from autogen.oai import openai_utils\n",
        "\n",
        "from google.colab import userdata\n"
      ],
      "metadata": {
        "id": "1CcoAdz-XNqc"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# define configs for model\n",
        "gpt_config = {\n",
        "    \"config_list\": [{\"model\": \"gpt-3.5-turbo\", \"api_key\": userdata.get('OPENAI_API_KEY')}],\n",
        "    \"timeout\": 120,\n",
        "    \"temperature\": 0.5,\n",
        "}\n",
        "\n",
        "gpt_vision_config = {\n",
        "    \"config_list\": [{\"model\": \"gpt-4o\", \"api_key\": userdata.get('OPENAI_API_KEY')}],\n",
        "    \"timeout\": 120,\n",
        "    \"temperature\": 0.5,\n",
        "}\n",
        "\n",
        "dalle_config = {\n",
        "    \"config_list\": [{\"model\": \"dall-e-2\", \"api_key\": userdata.get('OPENAI_API_KEY')}],\n",
        "    \"timeout\": 120,\n",
        "    \"temperature\": 0.5,\n",
        "}\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JxLLSCIHXoxg"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create image generator agent\n",
        "agent = autogen.ConversableAgent(\n",
        "    name=\"agent_with_dalle_capability\",\n",
        "    llm_config=gpt_vision_config,\n",
        "    max_consecutive_auto_reply=2,\n",
        "    human_input_mode=\"NEVER\"\n",
        ")\n",
        "\n",
        "# define image generation capability\n",
        "dalle_gen = generate_images.DalleImageGenerator(llm_config=dalle_config, resolution='256x256')\n",
        "image_gen_capability = generate_images.ImageGeneration(\n",
        "    image_generator=dalle_gen,\n",
        "    text_analyzer_llm_config=gpt_config,\n",
        "    verbosity=2\n",
        ")\n",
        "\n",
        "# add ability\n",
        "image_gen_capability.add_to_agent(agent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5JVjTw5ZAFe",
        "outputId": "399f604b-e33b-4f65-b422-9ffed9314a21"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[autogen.oai.client: 10-01 07:12:50] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autogen.oai.client:The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[autogen.oai.client: 10-01 07:12:50] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autogen.oai.client:The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_proxy = UserProxyAgent(\n",
        "    name=\"user_proxy\",\n",
        "    code_execution_config = False,\n",
        "    human_input_mode=\"NEVER\",\n",
        ")"
      ],
      "metadata": {
        "id": "njL2itLNcq9y"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reply = user_proxy.initiate_chat(agent, message = \"imagine kittens playing with a ball of yarn\", max_turns=1)\n",
        "reply"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsaYDSdhdBuf",
        "outputId": "6ea1ae18-8610-43c7-809b-92536cfd707a"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user_proxy (to agent_with_dalle_capability):\n",
            "\n",
            "imagine kittens playing with a ball of yarn\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "agent_with_dalle_capability (to user_proxy):\n",
            "\n",
            "That sounds adorable! Here is an image of kittens playing with a ball of yarn:\n",
            "\n",
            "![Kittens playing with a ball of yarn](https://i.imgur.com/n2h6M3V.jpg)\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatResult(chat_id=None, chat_history=[{'content': 'imagine kittens playing with a ball of yarn', 'role': 'assistant', 'name': 'user_proxy'}, {'content': 'That sounds adorable! Here is an image of kittens playing with a ball of yarn:\\n\\n![Kittens playing with a ball of yarn](https://i.imgur.com/n2h6M3V.jpg)', 'role': 'user', 'name': 'agent_with_dalle_capability'}], summary='That sounds adorable! Here is an image of kittens playing with a ball of yarn:\\n\\n![Kittens playing with a ball of yarn](https://i.imgur.com/n2h6M3V.jpg)', cost={'usage_including_cached_inference': {'total_cost': 0.0008100000000000001, 'gpt-4o-2024-05-13': {'cost': 0.0008100000000000001, 'prompt_tokens': 39, 'completion_tokens': 41, 'total_tokens': 80}}, 'usage_excluding_cached_inference': {'total_cost': 0}}, human_input=[])"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_images(sender, recipient):\n",
        "    images = []\n",
        "    all_messages = sender.chat_messages[recipient]\n",
        "\n",
        "    for message in reversed(all_messages):\n",
        "        # The GPT-4V format, where the content is an array of data\n",
        "        contents = message.get(\"content\", [])\n",
        "        for content in contents:\n",
        "            if isinstance(content, str):\n",
        "                continue\n",
        "            if content.get(\"type\", \"\") == \"image_url\":\n",
        "                img_data = content[\"image_url\"][\"url\"]\n",
        "                images.append(img_utils.get_pil_image(img_data))\n",
        "\n",
        "    if not images:\n",
        "        raise ValueError(\"No image data found in messages.\")\n",
        "\n",
        "    return images\n"
      ],
      "metadata": {
        "id": "ycqiAAg2kVgF"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = extract_images(user_proxy, agent)\n",
        "display(images[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "NkPhBhxLkdmR",
        "outputId": "a0bb721c-0b00-4bca-deae-4fdbfcc7864d"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "No image data found in messages.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-e4ef03795861>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_proxy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-54-29b91aed7333>\u001b[0m in \u001b[0;36mextract_images\u001b[0;34m(sender, recipient)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No image data found in messages.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No image data found in messages."
          ]
        }
      ]
    }
  ]
}